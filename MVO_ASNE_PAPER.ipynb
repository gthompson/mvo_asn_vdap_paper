{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "This Python3 notebook is used to generate most of the figures for the paper I am writing for Seismological Research Letters to describe capture, recovery and conversion of data from the Montserrat Volcano Observatory's analog seismic network (ASN) that was installed by the Volcano Disaster Assistance Program (VDAP) in 1995. Then ASN ran until December 2004 although the VDAP system was heavily modified on March 2nd, 2001, when PC-SEIS/XDETECT was swapped for the QNX/Seislog PC and BRAINS (SUDSPick, Hypo71) became obsolete.\n",
    "\n",
    "<h1>Instructions:</h1>\n",
    "\n",
    "<h3>Creation of CSVDB:</h3>\n",
    "<ol>\n",
    "    <li> cd to the root directory of this repo</li>\n",
    "    <li>run BIN/seisanwavdb2csv.py. This will create any missing ASNE_wavefilesYYYYMM.csv files. These have one line for each trace in each wavfile.</li>\n",
    "    <li>run count_traces_per_day.py. This will create ASNE_dailytraceid_wavfiles_df.csv. This has one line for each trace, every day. This is used to create the station and site on-time plots.</li>\n",
    "</ol>\n",
    "There can be missing days, where there was no data. This is what we use the fill_missing_days_in_df function for.\n",
    "\n",
    "<h3>SUDS/Hypo71 to Seisan database conversion</h3>\n",
    "Conversion of SUDS files to Miniseed, registering them into Seisan database in Nordic format (S-files), and then coverting PHA and PUN into Nordic format\n",
    "\n",
    "<ol>\n",
    "    <li>To learn how to convert a single event, use the vdap2seisanDB Python notebook. Each event must have either a WVM or DMX file (waveform data) and may also have a PHA file (phase pick readings) and a PUN file (Hypo71 summary file)</li>\n",
    "    <li>To convert an entire collection of WVM, DMX, PHA and PUN files, use vdap2seisanDB.py.</li> \n",
    "    <li>A special version of this is montserrat_vdap2seisanDB.py, which has additional complexities in it unique to how the MVO data have been recovered and converted at different times.</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Background:</h3>\n",
    "\n",
    "ASN waveform data was converted 3 times:\n",
    "\n",
    "    2001: \n",
    "    \n",
    "    I had copies of IRIG.EXE, DEMUX.EXE from UEA Belham project I think?. and I modified sudsei.for from Seisan so that I could drive it from the command line. So I constructed Perl script to loop over all year/month directories of SUDS data, and time correct, demutiplex and convert to Seisan. I thought I even went through and associated all IRIG files into the MVOE database. I wonder where my original copy of the MVO REA/MVOE_ database went?\n",
    "        \n",
    "    2015:\n",
    "        \n",
    "    I used PC-SUDS or WIN-SUDS (which contained IRIG.EXE and DEMUX.EXE) and CONVSEIS (SUDS2GSE.EXE). So a Perl script on a 32-bit Windows PC was able to convert everything to GSE-1 format. And then I wrote ObsPy script to convert everything to Miniseed. Miniseed files were organized into an Antelope database, and then included when building an SDS archive of the combined networks.\n",
    "    \n",
    "    2019: \n",
    "    \n",
    "    For the student project, I copied the ASN Miniseed event waveform data into a Seisan DB called ASNE. I also tested if conversion still worked, and on a 64-bit windows 10 laptop was able to run PC-SUDS DEMUX.EXE and SUD2MSED.EXE to convert some SUDS WVM to SUDS DMX and MiniSEED files.\n",
    "    \n",
    "    However, this was unsatisfying. So I decided to reconvert everything. I think this is in suds2seisandb.py and a similarly named notebook. I converted DMX files into SAC files and then into Miniseed files. If this did not work, I tried to find the corresponding GSE file from 2015 conversion. DMX and GSE files do not have any time conversions added, and I have not included any in the SAC or Miniseed version, so the ASNE_ Miniseed database on my Windows laptop is in exactly the same time zone as originally recorded. Was this recorded in local time (UTC - 4) or in UTC?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Issues discovered so far:</h3>\n",
    "\n",
    "* Sometimes with ASN WAV files we noticed that 1 file comes 1-s after another. Probably registered twice? Need to eliminate the duplicates.\n",
    "\n",
    "* The gaps in the WAV file ontime plot doesn't appear to correspond to the gaps found. Indeed, the gaps seem a lot less than show, e.g. early 2004.\n",
    "\n",
    "* I do not have a copy of the IRIG Seisan DB, which I made at MVO. It sounds like BGS have a copy, from reading Richard Luckett's report. I believe this was all the data from XDETECT that I originally converted in 2001, after it was phased out.\n",
    "\n",
    "* It appears that ASNE data (converted in 2015) are 4 hours behind SPN data (which we triggered in real-time by QNX, and appear to match DSN (MVO) data. This has only been seen in late March 2001 so far, which was after the transition. \n",
    "\n",
    "<b>Possibilities:</b>\n",
    "\n",
    "1. All ASNE WAV files have a timestamp that is too great by 4 hours. This seems possible if a 4 hour time correction was ever applied during my 2015 conversions. Were XDETECT files were timestamped with UTC or local time? Logbooks likely matched. So they might all have been UTC! If I have inadvertently added 4 hours to all ASNE WAV files, this means that new S-files that students are generating are off by 4 hours also. This is the worst case scenario. But it is also possible that XDETECT used localtime, in which case I did need to apply a 4 hour correction. Could I have applied the time correction twice?\n",
    "\n",
    "2. Only ASNE data after mid-March 2001 is off by 4 hours. I might have applied some blanket time correction to the data deriving from XDETECT (appropriate) and also from QNX Seislog (mistake). In this case I can probably throw away all ASN files after mid-March 2001 and use SPN files instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import datetime as dt\n",
    "\n",
    "def get_yticks(thisdf):\n",
    "    yticklabels = []\n",
    "    yticks=[]\n",
    "    i = 0\n",
    "    for index, row in thisdf.iterrows():\n",
    "        date = str(index)\n",
    "        if date[-2:]=='01':\n",
    "            yticklabels.append(date)\n",
    "            yticks.append(i)\n",
    "        i+=1\n",
    "    ystep = 1\n",
    "    if len(yticks)>15:\n",
    "        ystep=2  \n",
    "    if len(yticks)>25:\n",
    "        ystep=3\n",
    "    if len(yticks)>40:\n",
    "        ystep=4\n",
    "    if len(yticks)>60:\n",
    "        ystep=6\n",
    "    if len(yticks)>120:\n",
    "        ystep=12    \n",
    "    yticks = yticks[0::ystep]\n",
    "    yticklabels = yticklabels[0::ystep]\n",
    "    return (yticks, yticklabels)\n",
    "\n",
    "def ontime_plot(data,xticklabels,yticks,yticklabels):\n",
    "    # make ontime plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax.imshow(data, aspect='auto', cmap=plt.cm.gray, interpolation='nearest')\n",
    "    ax.imshow(np.transpose(data), aspect='auto', cmap=plt.cm.gray, interpolation='nearest')\n",
    "    #plt.xticks(np.arange(len(xticklabels)), xticklabels)\n",
    "    plt.yticks(np.arange(len(xticklabels)), xticklabels)\n",
    "    #ax.set_xticklabels(xticklabels, rotation = 90)\n",
    "    ax.set_yticklabels(xticklabels, rotation = 0)\n",
    "    #plt.yticks(yticks, yticklabels)\n",
    "    plt.xticks(yticks, yticklabels, rotation = 90)\n",
    "    plt.tight_layout()\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def numrunningperday_plot_deprecated(data, yticks, yticklabels):\n",
    "    fig = plt.figure()\n",
    "\n",
    "    rect_main = [0.1, 0.1, 0.5, 0.75]\n",
    "    rect_side = [0.7, 0.1, 0.2, 0.75]\n",
    "\n",
    "    ax_main = plt.axes(rect_main)\n",
    "    #ax1 = fig.add_subplot(121)\n",
    "    s = np.sum(data,axis=1)\n",
    "    ax_main.plot(s,'.',color='black',markersize=1)\n",
    "    #plt.plot(s,'.',color='black',markersize=1)\n",
    "    #plt.xticks(yticks, yticklabels)\n",
    "    ax_main.set_xticks(yticks, yticklabels)\n",
    "    #ax1.set_xticklabels(yticklabels, rotation = 90)\n",
    "    ax_main.set_xticklabels(yticklabels, rotation = 90)\n",
    "    #plt.ylabel('number\\noperational')\n",
    "    ax_main.set_ylabel('number\\noperational')\n",
    "    ax_main.tick_params(axis='both', which='major', labelsize=6)\n",
    "    #plt.grid()\n",
    "    ax_main.grid()\n",
    "    fig.tight_layout()  \n",
    "\n",
    "    ax_side = plt.axes(rect_side)\n",
    "    binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "    #ax2 = fig.add_subplot(122)\n",
    "    #plt.hist(s,bins=binedges,color='black',orientation='horizontal')\n",
    "    ax_side.hist(s,bins=binedges,color='black',orientation='horizontal')\n",
    "    return (fig, ax_main, s)\n",
    "\n",
    "def numrunningperday_plot(data, yticks, yticklabels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    s = np.sum(data,axis=1)\n",
    "    plt.plot(s,'.',color='black',markersize=1)\n",
    "    plt.xticks(yticks, yticklabels)\n",
    "    ax.set_xticks(yticks, yticklabels)\n",
    "    ax.set_xticklabels(yticklabels, rotation = 90)\n",
    "    ax.set_ylabel('number\\noperational')\n",
    "    #ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax.grid()\n",
    "    fig.tight_layout()  \n",
    "    \n",
    "    unique, counts = np.unique(s, return_counts=True)\n",
    "    countsdf = pd.DataFrame(np.transpose([unique, counts]),columns=['numsites',\"days\"])\n",
    "    numdays = countsdf['days'].sum()\n",
    "    percent = countsdf['days']/numdays * 100\n",
    "    countsdf['frequency']=percent\n",
    "    print(countsdf)\n",
    "    print('Total number of days is %.0f' % numdays)\n",
    "    \n",
    "    #ax_side.hist(s,bins=binedges,color='black',orientation='horizontal')\n",
    "    return (fig, ax, s)\n",
    "\n",
    "def intday2date(intday):\n",
    "    ymd = str(intday)\n",
    "    d = dt.date(int(ymd[0:4]),int(ymd[4:6]),int(ymd[6:8]))\n",
    "    return d\n",
    "\n",
    "def fill_missing_days_in_df(df, startday=19950726, endday=20041231):\n",
    "    # need to make sure that every date from start to end exists in sites_df\n",
    "    cols = list(df.columns)\n",
    "    lastd = intday2date(startday)\n",
    "    totalmissingdays = 0\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            ymd = str(index)\n",
    "        except:\n",
    "            continue\n",
    "        d = intday2date(ymd)\n",
    "        daysdiff = (d-lastd).days\n",
    "        if daysdiff > 1:\n",
    "            totalmissingdays += daysdiff - 1\n",
    "            print(daysdiff,' days missing between', lastd, ' and ', d)\n",
    "            for d1 in range(1,daysdiff):\n",
    "                missingday = lastd + dt.timedelta(days=d1)\n",
    "                missingindex = int(missingday.strftime('%Y%m%d'))\n",
    "                for col in cols:\n",
    "                    df.at[missingindex,col]=0\n",
    "                #missingdict = {'date':), \\\n",
    "                #    'BE':0, 'CP':0, 'GA':0, 'GH':0, 'GT':0, 'HR':0, 'JH':0, 'LG':0, \\\n",
    "                #    'LY':0, 'MVO':0, 'NEV':0, 'PL':0, 'RY':0, 'SA':0, 'SP':0, 'SS':0, \\\n",
    "                #    'VP':0, 'WH':0}\n",
    "            #df = df.append(missingdict, ignore_index=True)\n",
    "        lastd = d\n",
    "    endd = intday2date(endday)   \n",
    "    if lastd < endd:\n",
    "        daysdiff = (endd-lastd).days\n",
    "        for d1 in range(1,daysdiff+1):\n",
    "            missingday = lastd + dt.timedelta(days=d1)\n",
    "            missingindex = int(missingday.strftime('%Y%m%d'))\n",
    "            for col in cols:\n",
    "                df.at[missingindex,col]=0\n",
    "    return(df.sort_index())\n",
    "\n",
    "def translate_id2site(id):\n",
    "    if len(id)==4:\n",
    "        id = 'MV.%s' % id\n",
    "    thissite = id[4:6]\n",
    "    if id[3:7]=='LONG':\n",
    "        thissite = 'LG'\n",
    "    if id[3:5]=='MW':\n",
    "        thissite = 'WH'\n",
    "    if id[3:6]=='MVO':\n",
    "        thissite = 'MVO'\n",
    "    if id[4:7]=='NEV':\n",
    "        thissite = 'NEV'\n",
    "    if id[3:5]=='MP':\n",
    "        thissite = 'PL'\n",
    "    if id[3:6]=='MCH':\n",
    "        thissite = 'CP'\n",
    "    if id[3:6]=='MGT':\n",
    "        thissite = 'GA'\n",
    "    if id[3:7]=='IRIG' or id[3:7]=='TST1' or id[3:6]=='MRH':\n",
    "        thissite = ''\n",
    "    return thissite    \n",
    "\n",
    "def stationids2sites(uniqids):\n",
    "    sites = set()\n",
    "    for id in uniqids:\n",
    "        thissite = translate_id2site(id)\n",
    "        if not thissite:\n",
    "            continue\n",
    "        sites.add(thissite)\n",
    "    sites = sorted(sites)\n",
    "    return sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in CSV file which has the number of events for each traceid each day\n",
    "Columns are yyyymmdd traceid count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 traceid  count\n",
      "yyyymmdd                       \n",
      "19950701  MV.MVPZ.--.SHZ      0\n",
      "19950701  MV.MWHE.--.SHE      0\n",
      "19950701  MV.MCPT.--.SHZ      0\n",
      "19950701  MV.MVOV.--.SHY      0\n",
      "19950701  MV.MGAT.--.SHZ      0\n",
      "                 traceid  count\n",
      "yyyymmdd                       \n",
      "20041231  MV.MWHN.--.SHX      0\n",
      "20041231  MV.MHRE.--.SHX      0\n",
      "20041231  MV.MPVE.--.SHZ      0\n",
      "20041231  MV.MWHT.--.SHZ      0\n",
      "20041231  MV.MPLX.--.SHE      0\n"
     ]
    }
   ],
   "source": [
    "dailytraceidfile = 'DATA/ASNE_dailytraceid_wavfiles_df.csv'\n",
    "trace_df = pd.read_csv(dailytraceidfile)\n",
    "trace_df = trace_df[['yyyymmdd','traceid','count']]\n",
    "trace_df = trace_df.set_index('yyyymmdd')\n",
    "print(trace_df.head())\n",
    "print(trace_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MV.MBET.--.SHZ', 'MV.MCHT.--.SHZ', 'MV.MCPE.--.SHX', 'MV.MCPN.--.SHY', 'MV.MCPN.--.SHZ', 'MV.MCPT.--.SHZ', 'MV.MCPZ.--.SHY', 'MV.MCPZ.--.SHZ', 'MV.MGAT.--.SHZ', 'MV.MGHZ.--.SHZ', 'MV.MGT2.--.SHZ', 'MV.MHRE.--.SHX', 'MV.MHRE.--.SHZ', 'MV.MHRN.--.SHX', 'MV.MHRN.--.SHY', 'MV.MHRV.--.SHY', 'MV.MHRV.--.SHZ', 'MV.MJHL.--.SHZ', 'MV.MJHT.--.SHZ', 'MV.MLGL.--.SHE', 'MV.MLGL.--.SHZ', 'MV.MLGT.--.SHZ', 'MV.MLYT.--.SHZ', 'MV.MNEV.--.SHZ', 'MV.MPEW.--.SHE', 'MV.MPEW.--.SHZ', 'MV.MPLX.--.SHE', 'MV.MPLY.--.SHN', 'MV.MPLZ.--.SHZ', 'MV.MPNS.--.SHN', 'MV.MPVE.--.SHZ', 'MV.MRHT.--.SHZ', 'MV.MRYT.--.SHZ', 'MV.MSAT.--.SHZ', 'MV.MSPT.--.SHZ', 'MV.MSSE.--.SHX', 'MV.MSSN.--.SHY', 'MV.MSSZ.--.SHZ', 'MV.MVOE.--.SHZ', 'MV.MVON.--.SHX', 'MV.MVOV.--.SHY', 'MV.MVPE.--.SHE', 'MV.MVPE.--.SHX', 'MV.MVPN.--.SHY', 'MV.MVPV.--.SHZ', 'MV.MVPZ.--.SHZ', 'MV.MWEH.--.SHX', 'MV.MWEL.--.SHX', 'MV.MWH2.--.SHZ', 'MV.MWHE.--.SHE', 'MV.MWHE.--.SHX', 'MV.MWHE.--.SHY', 'MV.MWHN.--.SHN', 'MV.MWHN.--.SHX', 'MV.MWHN.--.SHY', 'MV.MWHT.--.SHZ', 'MV.MWHZ.--.SHZ', 'MV.MWNH.--.SHY', 'MV.MWNL.--.SHY', 'MV.MWZH.--.SHZ', 'MV.MWZL.--.SHZ']\n"
     ]
    }
   ],
   "source": [
    "uniqids = sorted(trace_df.traceid.unique())\n",
    "station_ids = list(filter(lambda x: 'MV.M' in x, uniqids))\n",
    "print(station_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32  days missing between 2002-09-30  and  2002-11-01\n",
      "31  days missing between 2003-03-31  and  2003-05-01\n"
     ]
    }
   ],
   "source": [
    "station_df = pd.DataFrame(columns = ['date'] + station_ids)\n",
    "for index, row in trace_df.iterrows():\n",
    "    #thisdate = row['yyyymmdd']\n",
    "    thisdate = index\n",
    "    thissta = row['traceid']\n",
    "    thiscount = row['count']\n",
    "    if np.isnan(thiscount):\n",
    "        thiscount = 0\n",
    "    station_df.at[thisdate,thissta] = thiscount\n",
    "        \n",
    "# Drop date column and fill missing days        \n",
    "station_df = station_df.drop(columns=['date'])\n",
    "station_df = fill_missing_days_in_df(station_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aren't these really independent NSLC's rather than just stations?\n",
    "station_bool_df = (station_df > 0) * 1\n",
    "station_bool_data = np.array(station_bool_df, dtype=float) # convert dataframe to numpy array\n",
    "xticklabels = station_ids\n",
    "(yticks, yticklabels) = get_yticks(station_bool_df)\n",
    "(fig, ax) = ontime_plot(1-station_bool_data,xticklabels,yticks,yticklabels)\n",
    "ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/ASNE_station_ontime.pdf',dpi=300)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So up to 17 channels. But mostly 10 (1266 days or 36.5%). 550 days with no data\n",
    "(yticks, yticklabels) = get_yticks(station_bool_df)\n",
    "(fig, ax, s) = numrunningperday_plot(station_bool_data, yticks, yticklabels)    \n",
    "plt.savefig('FIGURES/ASNE_numstationsperday.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(s,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational NSLCs')\n",
    "plt.ylabel('Number of days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop over rows in df, and create a new dataframe for sites, with a 1 to indicate if on for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqids = trace_df.traceid.unique()\n",
    "sites = stationids2sites(uniqids)\n",
    "sites_df = pd.DataFrame(columns = ['date'] + sorted(sites))\n",
    "for index, row in trace_df.iterrows():\n",
    "    #thisdate = row['yyyymmdd']\n",
    "    thisdate = index\n",
    "    thissite = translate_id2site(row['traceid'])\n",
    "    if not thissite:\n",
    "        continue\n",
    "    thiscount = row['count']\n",
    "    if np.isnan(thiscount):\n",
    "        thiscount = 0\n",
    "    currentcount = 0\n",
    "    try:\n",
    "        currentcount = sites_df.at[thisdate,thissite]\n",
    "    except:\n",
    "        pass\n",
    "    if np.isnan(currentcount):\n",
    "        currentcount = 0\n",
    "    if thiscount > currentcount:\n",
    "        sites_df.at[thisdate,thissite] = thiscount\n",
    "        \n",
    "# Drop date column and fill missing days        \n",
    "sites_df = sites_df.drop(columns=['date'])\n",
    "sites_df = fill_missing_days_in_df(sites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_bool_df = (sites_df > 0) * 1\n",
    "sites_bool_data = np.array(sites_bool_df, dtype=float) # convert dataframe to numpy array\n",
    "xticklabels = sites\n",
    "(yticks, yticklabels) = get_yticks(sites_bool_df)\n",
    "(fig, ax) = ontime_plot(1-sites_bool_data,xticklabels,yticks,yticklabels)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Site')\n",
    "fig.tight_layout()  \n",
    "plt.savefig('FIGURES/Figure7_ASNE_siteontime.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 524 days with no data, that's 15.2% of the 3446 days.\n",
    "# 6 sites was most common: 50.2% of the time (1729 days)\n",
    "(fig, ax, s) = numrunningperday_plot(sites_bool_data, yticks, yticklabels)    \n",
    "plt.savefig('FIGURES/Figure8_ASNE_numsitesperday.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(s,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/ASNE_sitesperday_histogram.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a plot made from the 2015 ASNE_ database. It appears I had 230,645 files in that.\n",
    "sites_data = np.array(sites_df) # convert dataframe to numpy array\n",
    "print(sites_data.shape)\n",
    "y = np.nanmax(sites_data,axis=1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Number of WAV files per day', color=color)\n",
    "ax1.plot(y, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "plt.xticks(yticks, yticklabels)\n",
    "ax1.set_xticklabels(yticklabels, rotation = 90)\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Cumulative number of WAV files', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(np.cumsum(y), color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#plt.show()\n",
    "plt.savefig('FIGURES/ASNE_numwavfilesperday.png',dpi=300)\n",
    "print('total # wavefiles = %d' % np.sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-create this figure from the countWAVfilesperday.py script run on the latest ASNE_ Miniseed file Seisan DB on Windows laptop. This was last modified on 2019/12/16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a plot made from the 2019 ASNE_ database on the Windows laptop. This only goes to March 1st, 2001.\n",
    "## so we append data from the 2015 ASNE_ database from March 2, 2001, which corresponds to SEISLOG system.\n",
    "## We have only 230,112 files. Not sure why we are missing 533 files from the VDAP system period.\n",
    "dailycounts_df = pd.read_csv('DATA/countWAVfiles.dat',delimiter=' ')\n",
    "dailycounts_df['Date']\n",
    "dateformatter = lambda x: x.replace('-','')\n",
    "dailycounts_df['Date'] = dailycounts_df['Date'].apply(dateformatter)\n",
    "dailycounts_df = dailycounts_df.set_index('Date')\n",
    "\n",
    "plt.close('all')\n",
    "#(yticks, yticklabels) = get_yticks(dailycounts_df)\n",
    "(yticks, yticklabels) = get_yticks(station_bool_df)\n",
    "y = dailycounts_df['Existing']\n",
    "y2 = dailycounts_df['ShouldBe']\n",
    "yall = np.nanmax(sites_data,axis=1)\n",
    "yall[0:len(y)]=y\n",
    "fig, ax1 = plt.subplots()\n",
    "color1 = '0.0' # greyscale\n",
    "color2 = '0.3'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('waveform files per day', color=color1)\n",
    "ax1.plot(range(len(yall)), yall, linewidth=0.1, color=color1)\n",
    "ax1.plot(range(len(y2)), y2, 'o',  markersize=0.2,  color=color2)\n",
    "#ax1.plot(range(len(y)), y, linewidth=0.2, color=color1)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('Cumulative', color=color1)  # we already handled the x-label with ax1\n",
    "ax2.plot(np.cumsum(yall), '--', color=normcolor, linewidth=1.5)\n",
    "ax2.plot(np.cumsum(y), color=normcolor, linewidth=1.5)\n",
    "ax2.plot(np.cumsum(y2), color=color1, linestyle = ':', linewidth = 1)\n",
    "ax2.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "plt.xticks(yticks, yticklabels)\n",
    "ax1.set_xticklabels(yticklabels, rotation = 90)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.savefig('FIGURES/Figure5_ASNEnumWaveformFilesPerDay_grey.pdf',dpi=300)\n",
    "print('total # Miniseed files from 1995/07/27 to 2001/03/01 = %d' % np.sum(y))\n",
    "print('total # missing Miniseed files from 1995/07/27 to 2001/03/01 = %d' % (np.sum(y2) - np.sum(y)) )\n",
    "print('total # Seisan files from 2001/03/02 to 2004/12/16 = %d' % (np.sum(yall) - np.sum(y)))\n",
    "print('total # waveform files = %d' % np.sum(yall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the amount of RSAM data captured\n",
    "\n",
    "RSAM_data_captured.m was written on iMac in Documents/MATLAB to make a summary of what fraction of RSAM data was collected for each station and for each day of the ASN. This generated files like RSAM_MBET.txt consisting of two columns: date (yyyymmdd) \\t  MBET\n",
    "\n",
    "Next we read in all these files, and create a dataframe from merging all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamtxtfiles = sorted(glob.glob('MATLAB/txtfiles/RSAM_????.txt'))\n",
    "count = 0\n",
    "for rsamfile in rsamtxtfiles:\n",
    "    rsamdf=pd.read_csv(rsamfile,sep='\\t')\n",
    "    if count == 0:\n",
    "        rsamdf_all = rsamdf\n",
    "    else:\n",
    "        rsamdf_all = pd.merge(rsamdf_all, rsamdf, on='date')\n",
    "    count += 1\n",
    "rsamdf_all = rsamdf_all.set_index('date')\n",
    "print(rsamdf_all.head())\n",
    "print(rsamdf_all.tail())\n",
    "rsamdf_all = fill_missing_days_in_df(rsamdf_all, startday=19950728, endday=20040110)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots summarizing how much RSAM data captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rsam = np.array(rsamdf_all, dtype=float) # convert dataframe to numpy array\n",
    "xticklabels = rsamdf_all.columns\n",
    "(yticks, yticklabels) = get_yticks(rsamdf_all)\n",
    "(fig, ax) = ontime_plot(1-data_rsam,xticklabels,yticks,yticklabels)\n",
    "\n",
    "plt.ylabel('Date')\n",
    "plt.xlabel('Station')\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAMontime.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booldata_rsam = (data_rsam > 0) * 1\n",
    "(fig, ax, s) = numrunningperday_plot(booldata_rsam, yticks, yticklabels)    \n",
    "plt.ylabel('Number of RSAM stations operational')\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAM_numstationsperday.png',dpi=300)\n",
    "#print(rsamdf_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(s,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/RSAM_numstationsperday_histogram.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamstations = list(xticklabels)\n",
    "rsamsites = stationids2sites(rsamstations)\n",
    "print(rsamsites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over rows in df, and create a new dataframe for sites, with a 1 to indicate if on for each day\n",
    "rsamsites_df = pd.DataFrame(columns = ['date'] + sorted(rsamsites))\n",
    "for index, row in rsamdf_all.iterrows():\n",
    "    for sta in rsamstations:\n",
    "        thisdate = index #row['date']\n",
    "        thissite = translate_id2site(sta)\n",
    "        #print(sta,thissite)\n",
    "        \n",
    "        if not thissite:\n",
    "            continue\n",
    "        thiscount = row[sta]\n",
    "        if np.isnan(thiscount):\n",
    "            thiscount = 0\n",
    "        currentcount = 0\n",
    "        try:\n",
    "            currentcount = rsamsites_df.at[thisdate,thissite]\n",
    "        except:\n",
    "            pass\n",
    "        if np.isnan(currentcount):\n",
    "            currentcount = 0\n",
    "        if thiscount > currentcount:\n",
    "            rsamsites_df.at[thisdate,thissite] = thiscount\n",
    "rsamsites_df = fill_missing_days_in_df(rsamsites_df, startday=19950728, endday=20040110)\n",
    "print(rsamsites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamsites_bool_df = (rsamsites_df > 0) * 1\n",
    "data_rsam_sites = np.array(rsamsites_bool_df, dtype=float) # convert dataframe to numpy array\n",
    "xticklabels = rsamsites\n",
    "(yticks, yticklabels) = get_yticks(rsamsites_bool_df)\n",
    "(fig, ax) = ontime_plot(1-data_rsam_sites[:,1:],xticklabels,yticks,yticklabels)\n",
    "plt.ylabel('Date')\n",
    "plt.xlabel('Site')\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAM_siteontime.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax, s) = numrunningperday_plot(data_rsam_sites, yticks, yticklabels)    \n",
    "plt.ylabel('Number of sites operational')\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAM_numsitesperday.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(s,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/RSAM_numsitesperday_histogram.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TILT data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilttxtfiles = sorted(glob.glob('MATLAB/txtfiles/TILT_????.txt'))\n",
    "count = 0\n",
    "for tiltfile in tilttxtfiles:\n",
    "    tiltdf=pd.read_csv(tiltfile,sep='\\t')\n",
    "    if count == 0:\n",
    "        tiltdf_all = tiltdf\n",
    "    else:\n",
    "        tiltdf_all = pd.merge(tiltdf_all, tiltdf, on='date')\n",
    "    count += 1\n",
    "tiltdf_all = tiltdf_all.set_index('date')\n",
    "print(tiltdf_all.head())\n",
    "print(tiltdf_all.tail())\n",
    "tiltdf_all = fill_missing_days_in_df(tiltdf_all, startday=19950727, endday=20001006)\n",
    "tiltdf_all = tiltdf_all.drop(columns=['CENT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiltdf_all = tiltdf_all[tiltdf_all.index <= 20001006] \n",
    "data_tilt = np.array(tiltdf_all, dtype=float) # convert dataframe to numpy array\n",
    "\n",
    "xticklabels = tiltdf_all.columns\n",
    "(yticks, yticklabels) = get_yticks(tiltdf_all)\n",
    "(fig, ax) = ontime_plot(1-data_tilt,xticklabels,yticks,yticklabels)\n",
    "\n",
    "plt.ylabel('Date')\n",
    "plt.xlabel('Station')\n",
    "fig.tight_layout()\n",
    "plt.savefig('TILT_ontime.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax, s) = numrunningperday_plot(data_tilt, yticks, yticklabels)    \n",
    "plt.ylabel('Number of TILT sites operational')\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/TILT_numsitesperday.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(s,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational TILT sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/TILT_numsitesperday_histogram.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to-do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASN Catalog data\n",
    "Make a plot of subplots of:\n",
    "(1) Number of cumulative located events\n",
    "(2) Cumulative energy plot\n",
    "(3) Number of cumulative pickfiles\n",
    "(4) Number of classified events vs WAV files (possibly just July 1995 - October 1996)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSAM EVENTS, ALARMS AND SSAM DATA\n",
    "\n",
    "Plots of:\n",
    "\n",
    "(1) cumulative RSAM alarms vs day (done)\n",
    "(2) cumulative RSAM events (or triggers?) vs day\n",
    "(3) SSAM data?\n",
    "\n",
    "Leave for later the task of making use of the amplitude data, or associating triggers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypodf3 = pd.DataFrame(columns=['otime','lat','lon','depth','mag'])\n",
    "#summaryfile = '/Volumes/data/Montserrat/MASTERING/VDAP/Hypocentres/summaryall_plus_willy.txt'\n",
    "summaryfile = 'DATA/summaryall_plus_willy.txt'\n",
    "#summaryfile = '/Users/thompsong/Documents/MATLAB/gdrive/data/MVO/seismicdata/copied_to_newton/Hypocentres/summaryall.txt'\n",
    "with open(summaryfile, 'r') as f1:\n",
    "    for str in f1:\n",
    "        try:\n",
    "            yy = float(str[0:2].strip())\n",
    "            mm = float(str[2:4].strip())\n",
    "            dd = float(str[4:6].strip())\n",
    "            hr = float(str[7:9].strip())\n",
    "            mi = float(str[9:11].strip())\n",
    "            secstr = str[12:17].strip()\n",
    "            secstr=secstr.replace(':','.')\n",
    "            sec = float(secstr.strip())\n",
    "            lat = float(str[17:20].strip())\n",
    "            mlat = float(str[21:26].strip())\n",
    "            lon = float(str[27:30].strip())\n",
    "            mlon = float(str[31:36].strip())\n",
    "            depth = float(str[37:43].strip())\n",
    "            magstr = str[45:49].strip()\n",
    "        except:\n",
    "            print('Failed on:')\n",
    "            print(str)\n",
    "            continue\n",
    "        try:\n",
    "            magstr = \"%3.1f\" % float(magstr)\n",
    "        except:\n",
    "            magstr = \" \" * 3\n",
    "        nphase = float(str[50:53].strip())\n",
    "        rms = float(str[62:66].strip())\n",
    "        dlat = float(lat) + float(mlat)/60.0\n",
    "        dlon = float(lon) + float(mlon)/60.0\n",
    "        if yy < 80:\n",
    "            cen = 20\n",
    "        else:\n",
    "            cen = 19\n",
    "        if (sec<60.0):\n",
    "            otime = dt.datetime(int(cen*100+yy),int(mm),int(dd),int(hr),int(mi),int(sec))\n",
    "        else:\n",
    "            otime = dt.datetime(int(cen*100+yy),int(mm),int(dd),int(hr),int(mi),0) + dt.timedelta(seconds=int(sec))\n",
    "        newrow = {'otime':otime, 'lat':dlat, 'lon':dlon, 'depth':depth, 'mag':magstr}\n",
    "        #print(newrow)\n",
    "        hypodf3 = hypodf.append(newrow , ignore_index=True)\n",
    "print(hypodf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypodf4 = hypodf3.sort_values(by='otime')\n",
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "x = hypodf4.otime\n",
    "y = np.cumsum(np.ones((len(x), 1)))\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative hypocenters', color='black')\n",
    "ax1.plot(x, y, '.', color='black', label='event alarms')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "#ax1.legend()\n",
    "\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "#plt.savefig('Hypocenters_timeseries.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countsfile = '/Users/thompsong/Desktop/Research/data_mastering_paper/MATLAB/spcounts_gt.csv'\n",
    "countsdf = pd.read_csv(countsfile)\n",
    "countsdf['date']=pd.Series([pd.to_datetime(date) for date in countsdf['date']])\n",
    "countsdf = countsdf.fillna(0)\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "#x = countsdf.date\n",
    "#lta=''\n",
    "#i=0\n",
    "#for a in x:\n",
    "#    ta=type(a)\n",
    "#    if ta!=lta:\n",
    "#        print(ta)\n",
    "#        print(i)\n",
    "#    lta=ta\n",
    "#    i+=1\n",
    "#print(x)\n",
    "# row 525 is bad\n",
    "countsdf=countsdf.drop([525])\n",
    "#print(countsdf.iloc[525])\n",
    "x = countsdf.date\n",
    "\n",
    "y = countsdf['volcano-tectonic'].apply(pd.to_numeric,errors='coerce')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative number of events', color='black')\n",
    "ax1.plot(x, np.cumsum(y), color='black', label='VT', linewidth=2)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "\n",
    "y2 = countsdf['long-period'].apply(pd.to_numeric,errors='coerce')\n",
    "ax1.plot(x, np.cumsum(y2), color='0.5', label='LP', linewidth=3)\n",
    "\n",
    "y3 = countsdf['hybrid'].apply(pd.to_numeric,errors='coerce')\n",
    "ax1.plot(x, np.cumsum(y3), '--', color='black', label='Hybrid', linewidth=2)\n",
    "\n",
    "y4 = countsdf['rockfall'].apply(pd.to_numeric,errors='coerce')\n",
    "ax1.plot(x, np.cumsum(y4), ':', color='0.5', label='Rockfall', linewidth=2)\n",
    "\n",
    "x2 = hypodf4.otime\n",
    "y5 = np.ones((len(x2), 1))\n",
    "#ax1.set_xlabel('Date')\n",
    "#ax1.set_ylabel('Cumulative hypocenters', color='black')\n",
    "ax1.plot(x2, np.cumsum(y5), color='0.25', label='hypocenters', linewidth=1)\n",
    "#plt.gcf().autofmt_xdate()\n",
    "\n",
    "# find hypocenters with magnitudes\n",
    "c = 0\n",
    "i = 0\n",
    "ind = []\n",
    "for mstr in hypodf4.mag:\n",
    "    try:\n",
    "        m = float(mstr)\n",
    "        c+=1\n",
    "        ind.append(i)\n",
    "    except:\n",
    "        pass\n",
    "    i+=1\n",
    "print(c)\n",
    "magdf = hypodf4.iloc[ind]\n",
    "x6 = magdf.otime\n",
    "y6 = np.ones((len(x6), 1))\n",
    "#ax1.set_xlabel('Date')\n",
    "#ax1.set_ylabel('Cumulative hypocenters', color='black')\n",
    "ax1.plot(x6, np.cumsum(y6), ':', color='0.25', label='magnitudes', linewidth=1)\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "#plt.grid()\n",
    "fig.tight_layout()\n",
    "\n",
    "print('total VTs: %d' % np.sum(y))\n",
    "print('total LPs: %d' % np.sum(y2))\n",
    "print('total Hybrids: %d' % np.sum(y3))\n",
    "print('total Rockfalls: %d' % np.sum(y4))\n",
    "print('total Hypocenters: %d' % np.sum(y5))\n",
    "print('total Magnitudes: %d' % np.sum(y6))\n",
    "plt.savefig('FIGURES/Figure6_eventcounts.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(magdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsameventalarmsdf = pd.read_csv('rsam_events_alarms.csv')\n",
    "rsameventalarmsdf.DATE = pd.Series([pd.to_datetime(date) for date in rsameventalarmsdf.DATE])\n",
    "rsamtremoralarmsdf = pd.read_csv('rsam_tremor_alarms.csv')\n",
    "rsamtremoralarmsdf.DATE = pd.Series([pd.to_datetime(date) for date in rsamtremoralarmsdf.DATE])\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "x = rsameventalarmsdf.DATE\n",
    "y = np.cumsum(np.ones((len(x), 1)))\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative RSAM alarms', color='black')\n",
    "ax1.plot(x, y, '.', color='black', label='event alarms')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "x2 = rsamtremoralarmsdf.DATE\n",
    "y2 = np.cumsum(np.ones((len(x2), 1)))\n",
    "ax1.plot(x2, y2, '.', color='gray', label='tremor alarms')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAMalarms.pdf',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "x = rsameventalarmsdf.DATE\n",
    "y = rsameventalarmsdf['MLYT.DATA']+rsameventalarmsdf['MJHT.DATA']+ \\\n",
    "rsameventalarmsdf['MRYT.DATA']+rsameventalarmsdf['MLGT.DATA']+ \\\n",
    "rsameventalarmsdf['MWHZ.DATA']+rsameventalarmsdf['MGHZ.DATA']\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative RSAM alarm amplitude', color='black')\n",
    "ax1.plot(x, np.cumsum(y), '.', color='black')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "x2 = rsamtremoralarmsdf.DATE\n",
    "y2 = rsamtremoralarmsdf['MLYT.DATA']+rsamtremoralarmsdf['MJHT.DATA']+ \\\n",
    "rsamtremoralarmsdf['MRYT.DATA']+rsamtremoralarmsdf['MLGT.DATA']+ \\\n",
    "rsamtremoralarmsdf['MWHZ.DATA']+rsamtremoralarmsdf['MGHZ.DATA']\n",
    "ax1.plot(x2, np.cumsum(y2), '.', color='gray')\n",
    "plt.savefig('FIGURES/RSAMalarms_amplitudes.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "x = rsameventalarmsdf.DATE\n",
    "# all stations\n",
    "y = rsameventalarmsdf['MLYT.THRESH']+rsameventalarmsdf['MJHT.THRESH']+ \\\n",
    "rsameventalarmsdf['MRYT.THRESH']+rsameventalarmsdf['MLGT.THRESH']+ \\\n",
    "rsameventalarmsdf['MWHZ.THRESH']+rsameventalarmsdf['MGHZ.THRESH']\n",
    "# one station\n",
    "y = rsameventalarmsdf['MRYT.THRESH'] \n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative RSAM alarm threshold', color='black')\n",
    "ax1.plot(x, y, color='black')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# tremor alarms\n",
    "#x2 = rsamtremoralarmsdf.DATE\n",
    "#y2 = rsamtremoralarmsdf['MLYT.THRESH']+rsamtremoralarmsdf['MJHT.THRESH']+ \\\n",
    "#rsamtremoralarmsdf['MRYT.THRESH']+rsamtremoralarmsdf['MLGT.THRESH']+ \\\n",
    "#rsamtremoralarmsdf['MWHZ.THRESH']+rsamtremoralarmsdf['MGHZ.THRESH']\n",
    "#y = rsamtremoralarmsdf['MLGT.THRESH']\n",
    "#ax1.plot(x2, y2, color='gray')\n",
    "\n",
    "plt.savefig('FIGURES/RSAMalarms_thresholds.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSN data\n",
    "\n",
    "With the DSN data, we want to do similar things regarding DSNC_, BSAM and DSNE_ (MVOE_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged data\n",
    "(1) Minutes per day of data at each site?\n",
    "(2) got to merge the ASN and DSN catalogs properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
